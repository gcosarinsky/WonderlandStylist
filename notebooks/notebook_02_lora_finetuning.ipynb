{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56bbd58d",
   "metadata": {},
   "source": [
    "## Notebook 02: Fine-Tuning con Unsloth\n",
    "\n",
    "Este notebook implementa el fine-tuning de un modelo Llama-3 usando LoRA (Low-Rank Adaptation) para crear el estilo de Alicia en el País de las Maravillas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a1a41",
   "metadata": {},
   "source": [
    "### 1. Instalación de herramientas \n",
    "\n",
    "**Nota:** Esta celda está diseñada para ejecutarse en **Google Colab**. Si trabajas en **local**, asegúrate de tener estas librerías ya instaladas en tu entorno Python.\n",
    "\n",
    "⚠️ **Requisito:** Se necesita una **GPU con CUDA** para ejecutar este notebook. En Colab, usa un runtime con GPU (Runtime > Change runtime type > T4 GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20057c2",
   "metadata": {},
   "source": [
    "### 2. Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf25b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a704801",
   "metadata": {},
   "source": [
    "### 3. Cargar modelo Llama-3 (optimizado para 4-bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bfc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf80123",
   "metadata": {},
   "source": [
    "### 4. Configurar LoRA\n",
    "\n",
    "Solo entrenamos el 1-2% de los parámetros usando LoRA (Low-Rank Adaptation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c86e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fa981",
   "metadata": {},
   "source": [
    "### 5. Formatear el Dataset\n",
    "\n",
    "Formato Instrucción-Input-Respuesta (formato Alpaca) para el prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af96ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = \"\"\"A continuación se muestra una instrucción que describe una tarea.\n",
    "Escribe una respuesta que complete adecuadamente la petición.\n",
    "\n",
    "### Instrucción:\n",
    "{}\n",
    "\n",
    "### Entrada:\n",
    "{}\n",
    "\n",
    "### Respuesta:\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    # The instruction is constant for all examples in the batch.\n",
    "    constant_instruction = \"Reescribe el siguiente texto con el estilo de Alicia en el País de las Maravillas.\"\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    # Iterate over inputs and outputs, applying the constant instruction to each pair.\n",
    "    for input_text, output_text in zip(inputs, outputs):\n",
    "        text = prompt_format.format(constant_instruction, input_text, output_text)\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"/content/synthetic_alicia.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f887e9",
   "metadata": {},
   "source": [
    "### 6. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    tokenizer = tokenizer, \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fbaab4",
   "metadata": {},
   "source": [
    "### 7. Exportar a GGUF para Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0772b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\"model_alicia\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392cf3aa",
   "metadata": {},
   "source": [
    "### 8. Cómo usar el modelo con Ollama\n",
    "\n",
    "Una vez exportado el modelo, necesitas cargarlo en Ollama para usarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b3f38",
   "metadata": {},
   "source": [
    "#### Paso 1: Crear un Modelfile\n",
    "\n",
    "Crea un archivo llamado `Modelfile` en tu directorio local con el siguiente contenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbb3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Modelfile\n",
    "FROM ./model_alicia-Q4_K_M.gguf\n",
    "\n",
    "TEMPLATE \"\"\"A continuación se muestra una instrucción que describe una tarea.\n",
    "Escribe una respuesta que complete adecuadamente la petición.\n",
    "\n",
    "### Instrucción:\n",
    "Reescribe el siguiente texto con el estilo de Alicia en el País de las Maravillas.\n",
    "\n",
    "### Entrada:\n",
    "{{ .Prompt }}\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER stop \"###\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001368a",
   "metadata": {},
   "source": [
    "#### Paso 2: Importar el modelo a Ollama\n",
    "\n",
    "Ejecuta este comando en tu terminal local (no en Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893178d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En terminal local:\n",
    "# ollama create alicia -f Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd665a",
   "metadata": {},
   "source": [
    "#### Paso 3: Usar el modelo\n",
    "\n",
    "Desde terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face4f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En terminal:\n",
    "# ollama run alicia \"Era una tarde soleada y los niños jugaban en el parque\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b4002",
   "metadata": {},
   "source": [
    "O desde Python con la API de Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post('http://localhost:11434/api/generate', \n",
    "    json={\n",
    "        'model': 'alicia',\n",
    "        'prompt': 'Era una tarde soleada y los niños jugaban en el parque',\n",
    "        'stream': False\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.json()['response'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
