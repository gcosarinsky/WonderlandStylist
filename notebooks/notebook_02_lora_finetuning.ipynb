{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56bbd58d",
   "metadata": {},
   "source": [
    "## Notebook 02: Fine-Tuning con Unsloth\n",
    "\n",
    "Este notebook implementa el fine-tuning de un modelo Llama-3 usando LoRA (Low-Rank Adaptation) para crear el estilo de Alicia en el Pa√≠s de las Maravillas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a1a41",
   "metadata": {},
   "source": [
    "### 1. Instalaci√≥n de herramientas \n",
    "\n",
    "**Nota:** Esta celda est√° dise√±ada para ejecutarse en **Google Colab**. Si trabajas en **local**, aseg√∫rate de tener estas librer√≠as ya instaladas en tu entorno Python.\n",
    "\n",
    "‚ö†Ô∏è **Requisito:** Se necesita una **GPU con CUDA** para ejecutar este notebook. En Colab, usa un runtime con GPU (Runtime > Change runtime type > T4 GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20057c2",
   "metadata": {},
   "source": [
    "### 2. Importar librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf25b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a704801",
   "metadata": {},
   "source": [
    "### 3. Cargar modelo Llama-3 (optimizado para 4-bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bfc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth\\Qwen3-0.6B-Base-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc5f19",
   "metadata": {},
   "source": [
    "### 3.5. Prueba del modelo base (antes del fine-tuning)\n",
    "\n",
    "Vamos a probar el modelo base con algunas frases para ver c√≥mo responde antes del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3870abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frases de prueba\n",
    "test_phrases = [\n",
    "    \"Era una tarde soleada y los ni√±os jugaban en el parque\",\n",
    "    \"El gato dorm√≠a pl√°cidamente en el sof√°\",\n",
    "    \"Mar√≠a caminaba por la calle cuando empez√≥ a llover\"\n",
    "]\n",
    "\n",
    "# Preparar el modelo para inferencia\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Formato del prompt (el mismo que usaremos en el entrenamiento)\n",
    "prompt_template = \"\"\"A continuaci√≥n se muestra una instrucci√≥n que describe una tarea.\n",
    "Escribe una respuesta que complete adecuadamente la petici√≥n.\n",
    "\n",
    "### Instrucci√≥n:\n",
    "Reescribe el siguiente texto con el estilo de Alicia en el Pa√≠s de las Maravillas.\n",
    "\n",
    "### Entrada:\n",
    "{}\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESULTADOS DEL MODELO BASE (SIN FINE-TUNING)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, phrase in enumerate(test_phrases, 1):\n",
    "    print(f\"\\nüìù Frase {i}: {phrase}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Crear el prompt\n",
    "    prompt = prompt_template.format(phrase)\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generar respuesta\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.7, top_p=0.9)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraer solo la respuesta (despu√©s de \"### Respuesta:\")\n",
    "    if \"### Respuesta:\" in response:\n",
    "        response = response.split(\"### Respuesta:\")[-1].strip()\n",
    "    \n",
    "    print(f\"ü§ñ Respuesta: {response}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf80123",
   "metadata": {},
   "source": [
    "### 4. Configurar LoRA\n",
    "\n",
    "Solo entrenamos el 1-2% de los par√°metros usando LoRA (Low-Rank Adaptation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c86e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fa981",
   "metadata": {},
   "source": [
    "### 5. Formatear el Dataset\n",
    "\n",
    "Formato Instrucci√≥n-Input-Respuesta (formato Alpaca) para el prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af96ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = \"\"\"A continuaci√≥n se muestra una instrucci√≥n que describe una tarea.\n",
    "Escribe una respuesta que complete adecuadamente la petici√≥n.\n",
    "\n",
    "### Instrucci√≥n:\n",
    "{}\n",
    "\n",
    "### Entrada:\n",
    "{}\n",
    "\n",
    "### Respuesta:\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    # The instruction is constant for all examples in the batch.\n",
    "    constant_instruction = \"Reescribe el siguiente texto con el estilo de Alicia en el Pa√≠s de las Maravillas.\"\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    # Iterate over inputs and outputs, applying the constant instruction to each pair.\n",
    "    for input_text, output_text in zip(inputs, outputs):\n",
    "        text = prompt_format.format(constant_instruction, input_text, output_text)\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"/content/synthetic_alicia.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f887e9",
   "metadata": {},
   "source": [
    "### 6. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    tokenizer = tokenizer, \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982771a",
   "metadata": {},
   "source": [
    "### 6.5. Prueba del modelo fine-tuneado\n",
    "\n",
    "Ahora vamos a probar el modelo entrenado con las mismas frases para comparar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar el modelo entrenado para inferencia\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESULTADOS DEL MODELO FINE-TUNEADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, phrase in enumerate(test_phrases, 1):\n",
    "    print(f\"\\nüìù Frase {i}: {phrase}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Crear el prompt\n",
    "    prompt = prompt_template.format(phrase)\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generar respuesta\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.7, top_p=0.9)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraer solo la respuesta (despu√©s de \"### Respuesta:\")\n",
    "    if \"### Respuesta:\" in response:\n",
    "        response = response.split(\"### Respuesta:\")[-1].strip()\n",
    "    \n",
    "    print(f\"‚ú® Respuesta: {response}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fbaab4",
   "metadata": {},
   "source": [
    "### 7. Guardar el modelo\n",
    "\n",
    "Tenemos dos opciones: guardar solo los adaptadores LoRA (r√°pido) o exportar a GGUF (lento pero necesario para Ollama)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04786e",
   "metadata": {},
   "source": [
    "#### 7.1. Guardar adaptadores LoRA (R√ÅPIDO - Recomendado primero)\n",
    "\n",
    "Esta opci√≥n es **muy r√°pida** (segundos) y guarda solo los pesos entrenados (adaptadores LoRA). El archivo resultante pesa solo unos pocos MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e9446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar solo los adaptadores LoRA (muy r√°pido)\n",
    "model.save_pretrained(\"lora_model_alicia\")\n",
    "tokenizer.save_pretrained(\"lora_model_alicia\")\n",
    "\n",
    "print(\"‚úÖ Adaptadores LoRA guardados en 'lora_model_alicia/'\")\n",
    "print(\"üì¶ Tama√±o: ~50-100 MB\")\n",
    "print(\"‚è±Ô∏è Tiempo: unos segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f63c107",
   "metadata": {},
   "source": [
    "#### 7.2. Exportar a GGUF para Ollama (LENTO - Solo si vas a usar Ollama)\n",
    "\n",
    "‚ö†Ô∏è **ADVERTENCIA**: Este proceso puede tardar **10-30 minutos** o m√°s. Solo ejecuta esta celda si necesitas usar el modelo en Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0772b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este proceso puede tardar 10-30 minutos\n",
    "print(\"‚è≥ Iniciando exportaci√≥n a GGUF...\")\n",
    "print(\"‚ö†Ô∏è Este proceso puede tardar entre 10-30 minutos\")\n",
    "\n",
    "model.save_pretrained_gguf(\"model_alicia\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "\n",
    "print(\"‚úÖ Modelo exportado a GGUF en 'model_alicia/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392cf3aa",
   "metadata": {},
   "source": [
    "### 8. C√≥mo cargar el modelo guardado\n",
    "\n",
    "Dependiendo de c√≥mo guardaste el modelo, tienes diferentes opciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de235c11",
   "metadata": {},
   "source": [
    "#### 8.1. Opci√≥n A: Cargar adaptadores LoRA en Python (si guardaste con 7.1)\n",
    "\n",
    "Puedes cargar los adaptadores LoRA directamente en Python para hacer inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de c√≥mo cargar el modelo con adaptadores LoRA guardados\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Cargar el modelo base + adaptadores LoRA\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"lora_model_alicia\",  # Carpeta donde guardamos\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Preparar para inferencia\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Ya puedes usar el modelo\n",
    "prompt = \"\"\"A continuaci√≥n se muestra una instrucci√≥n que describe una tarea.\n",
    "Escribe una respuesta que complete adecuadamente la petici√≥n.\n",
    "\n",
    "### Instrucci√≥n:\n",
    "Reescribe el siguiente texto con el estilo de Alicia en el Pa√≠s de las Maravillas.\n",
    "\n",
    "### Entrada:\n",
    "Era una tarde soleada\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d6c196",
   "metadata": {},
   "source": [
    "#### 8.2. Opci√≥n B: Usar con Ollama (si exportaste a GGUF en 7.2)\n",
    "\n",
    "Una vez exportado el modelo a GGUF, necesitas cargarlo en Ollama para usarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b3f38",
   "metadata": {},
   "source": [
    "##### Paso 1: Crear un Modelfile\n",
    "\n",
    "Crea un archivo llamado `Modelfile` en tu directorio local con el siguiente contenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbb3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Modelfile\n",
    "FROM ./model_alicia-Q4_K_M.gguf\n",
    "\n",
    "TEMPLATE \"\"\"A continuaci√≥n se muestra una instrucci√≥n que describe una tarea.\n",
    "Escribe una respuesta que complete adecuadamente la petici√≥n.\n",
    "\n",
    "### Instrucci√≥n:\n",
    "Reescribe el siguiente texto con el estilo de Alicia en el Pa√≠s de las Maravillas.\n",
    "\n",
    "### Entrada:\n",
    "{{ .Prompt }}\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER stop \"###\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001368a",
   "metadata": {},
   "source": [
    "##### Paso 2: Importar el modelo a Ollama\n",
    "\n",
    "Ejecuta este comando en tu terminal local (no en Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893178d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En terminal local:\n",
    "# ollama create alicia -f Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd665a",
   "metadata": {},
   "source": [
    "##### Paso 3: Usar el modelo\n",
    "\n",
    "Desde terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face4f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En terminal:\n",
    "# ollama run alicia \"Era una tarde soleada y los ni√±os jugaban en el parque\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b4002",
   "metadata": {},
   "source": [
    "O desde Python con la API de Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post('http://localhost:11434/api/generate', \n",
    "    json={\n",
    "        'model': 'alicia',\n",
    "        'prompt': 'Era una tarde soleada y los ni√±os jugaban en el parque',\n",
    "        'stream': False\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.json()['response'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
