# üé© Wonderland Stylist

Un proyecto de IA que transforma texto plano al estilo literario de "Alicia en el Pa√≠s de las Maravillas" de Lewis Carroll, utilizando un modelo LLM optimizado para ejecutarse con recursos de hardware limitados.

## üìã Descripci√≥n

Este proyecto implementa un sistema completo de fine-tuning y deployment de un modelo de lenguaje para transformar frases cotidianas en texto con el estilo caracter√≠stico de Alicia en el Pa√≠s de las Maravillas. El modelo final es ligero y eficiente, dise√±ado para funcionar en hardware modesto.

## üèóÔ∏è Arquitectura del Proyecto

### 1. Generaci√≥n de Datos Sint√©ticos
- **Modelo**: `llama3.2` mediante Ollama
- **Proceso**: Generaci√≥n de pares de texto (normal ‚Üí estilo Alicia)
- **Salida**: Dataset sint√©tico en formato JSONL

### 2. Fine-tuning con LoRA
- **Framework**: Unsloth
- **Modelo base**: Qwen3 0.6B
- **T√©cnica**: LoRA (Low-Rank Adaptation)
- **Formato final**: GGUF optimizado (Q4_K_M)

### 3. Inferencia
- **Runtime**: Ollama
- **Interfaz**: Gradio Web UI
- **Modelo**: `Qwen3-0.6B-Base.Q4_K_M_Alicia.gguf`

## üìÅ Estructura del Proyecto

```
WonderlandStylist/
‚îú‚îÄ‚îÄ api/                    # API y aplicaci√≥n web
‚îÇ   ‚îî‚îÄ‚îÄ gradio_app.py      # Interfaz Gradio
‚îú‚îÄ‚îÄ data/                   # Datos de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ synthetic_alicia.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ chunks_nltk.txt
‚îÇ   ‚îú‚îÄ‚îÄ create_chunks.py
‚îÇ   ‚îî‚îÄ‚îÄ raw/               # Texto original de Alicia
‚îú‚îÄ‚îÄ model/                  # Modelos y configuraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ Modelfile.alicia
‚îÇ   ‚îî‚îÄ‚îÄ Qwen3-0.6B-Base.Q4_K_M_Alicia.gguf
‚îú‚îÄ‚îÄ notebooks/              # Notebooks de desarrollo
‚îÇ   ‚îú‚îÄ‚îÄ notebook_01_gen_synthetic_data.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ notebook_02_lora_finetuning.ipynb
‚îú‚îÄ‚îÄ scripts/                # Scripts de utilidad y testing
‚îÇ   ‚îú‚îÄ‚îÄ run_gradio_app.py
‚îÇ   ‚îú‚îÄ‚îÄ test_base_model.py
‚îÇ   ‚îî‚îÄ‚îÄ view_dataset.py
‚îî‚îÄ‚îÄ fastapi_tests/          # Tests y experimentos con FastAPI
```

### Requisitos
- Python 3.8+
- Ollama instalado
- CUDA (opcional, para aceleraci√≥n GPU)

## üíª Uso

### 1. Generar Dataset Sint√©tico
Ejecutar el script `data/create_chunks.py` para fragmentar el texto original de Alicia usando NLTK. Luego, ejecutar el notebook `notebook_01_gen_synthetic_data.ipynb` para transformar cada fragmento al estilo plano usando llama3.2. Este proceso inverso genera el dataset de entrenamiento en formato instruction-input-output, guardado en `data/synthetic_alicia.jsonl`. 
El prompt utilizado sigue el siguiente template:

> ### Instrucci√≥n:
> 
> Reescribe el siguiente texto con el estilo de Alicia en el Pa√≠s de las Maravillas.
> 
> 
> 
> ### Entrada:
> 
> {frase}
> 
> 
> 
> ### Respuesta:


### 2. Fine-tuning del Modelo
Ejecutar el notebook `notebook_02_lora_finetuning.ipynb` para entrenar el modelo Qwen3 0.6B con LoRA. 

**Requisitos:**
- GPU compatible con CUDA
- Librer√≠a Unsloth instalada

**Proceso:**
El notebook realiza fine-tuning utilizando Unsloth, que incluye optimizaciones para acelerar el entrenamiento. Al finalizar, genera autom√°ticamente el modelo cuantizado en formato GGUF (Q4_K_M) y lo guarda en:

```
model/Qwen3-0.6B-Base.Q4_K_M_Alicia.gguf
```

### 3. Cargar el Modelo en Ollama

Una vez generado el archivo GGUF, es necesario crear un modelo personalizado en Ollama utilizando el archivo `model\Modelfile.alicia`, que define el prompt del sistema y los par√°metros de inferencia.

**Opci√≥n 1: Usar el script automatizado**

Ejecutar el script correspondiente a tu sistema operativo:

**Windows:**
```bash
setup_ollama_model.bat
```

**Linux/Mac:**
```bash
chmod +x setup_ollama_model.sh
./setup_ollama_model.sh
```

**Opci√≥n 2: Pasos manuales**

1. Navegar al directorio del modelo:
```bash
cd model
```

2. Crear el modelo personalizado en Ollama:
```bash
ollama create alicia -f Modelfile.alicia
```

3. Iniciar el modelo en modo servidor:
```bash
ollama run alicia
```

**Nota:** El modelo debe permanecer en ejecuci√≥n para que la aplicaci√≥n Python pueda enviar requests de inferencia a trav√©s de la API de Ollama.


### 4. Ejecutar la Interfaz Gradio

```bash
python scripts/run_gradio_app.py
```

O directamente:

```bash
python api/gradio_app.py
```
## üß™ Testing y Validaci√≥n

### Probar el Modelo Base (Sin Fine-tuning)

Para comparar el comportamiento del modelo Qwen3:0.6b antes del fine-tuning:

```bash
python scripts/test_base_model.py
```

Este script ejecuta m√∫ltiples prompts de prueba, incluyendo el template utilizado en el modelo final. Los resultados se guardan autom√°ticamente con timestamp.

**Ejemplo de salida:** `scripts/test_base_model_output_20260203_155348.txt`

### Inspeccionar el Dataset de Entrenamiento

Para visualizar los pares de entrenamiento generados:

```bash
python scripts/view_dataset.py
```

Este script permite explorar el dataset `synthetic_alicia.jsonl` ejemplo por ejemplo, mostrando el formato instruction-input-output utilizado durante el fine-tuning.


## üõ†Ô∏è Tecnolog√≠as Utilizadas

- **Ollama**: Runtime para modelos LLM
- **Unsloth**: Framework de fine-tuning optimizado
- **Gradio**: Framework para interfaces web interactivas
- **Qwen3 0.6B**: Modelo base ligero
- **LoRA**: T√©cnica de fine-tuning eficiente
- **GGUF**: Formato de modelo cuantizado

## üìä Caracter√≠sticas del Modelo

- **Tama√±o**: ~400MB (cuantizado Q4_K_M)
- **Par√°metros**: 0.6B
- **Requisitos de RAM**: ~2-4GB

## üîç Resultados y Conclusiones

El modelo presenta limitaciones significativas en la generaci√≥n de texto con el estilo de Alicia en el Pa√≠s de las Maravillas. A continuaci√≥n se muestran ejemplos representativos del comportamiento actual:

**Input de prueba:**
```
El gato dorm√≠a pl√°cidamente en el sof√°
```

**Outputs generados:**

1. *"El gato dorm√≠a pl√°cidamente en el sof√°, como si el mundo fuera una estatua."*
    - A√±ade una comparaci√≥n, pero no captura el estilo narrativo caracter√≠stico.

2. *"A Alicia le gustaba la idea de que el gato dorm√≠a pl√°cidamente en el sof√°, porque cre√≠a que as√≠ ser√≠a el gato que dorm√≠a en su propio cuarto."*
    - Incorpora el personaje de Alicia, pero la estructura es confusa y redundante.

3. *"El gato durmiendo en el sof√°, no parece muy lejos de aqu√≠."*
    - Apenas modifica el texto original, perdiendo coherencia.

4. *"¬´‚ÄîNo te lo digo en serio‚Äî. ‚ÄîDe nuevo, le estaba diciendo a Alicia que no pod√≠a dejarlo con ella. ‚ÄîNo, es una vez en su vida que un gato dura un d√≠a y no m√°s.¬ª"*
    - Genera di√°logos sin contexto y con poca coherencia sem√°ntica.

### An√°lisis y Mejoras Propuestas

**Problemas identificados:**

- El modelo no logra capturar consistentemente el tono y las estructuras narrativas caracter√≠sticas de Lewis Carroll
- Las transformaciones tienden a ser superficiales o a generar texto incoherente
- La estrategia de generaci√≥n de datos sint√©ticos (invertir texto estilizado ‚Üí texto plano) puede no ser √≥ptima

**L√≠neas de mejora:**

1. **Revisi√≥n del dataset de entrenamiento:**
    - Aumentar la cantidad de ejemplos de entrenamiento
    - Validar manualmente la calidad de los pares sint√©ticos generados
    - Considerar incluir ejemplos reales de transformaciones de estilo

2. **Optimizaci√≥n del prompt template:**
    - Experimentar con instrucciones m√°s espec√≠ficas
    - Ajustar los par√°metros de temperatura y top-p durante la inferencia

3. **Alternativas de arquitectura:**
    - Probar modelos base de mayor capacidad (1.5B-3B par√°metros)
    - Considerar enfoques de transfer learning desde modelos pre-entrenados en tareas similares


## üìù Notas

- El modelo est√° optimizado para espa√±ol
- Los datos de entrenamiento provienen del texto original "Alicia en el Pa√≠s de las Maravillas" de Lewis Carroll
- La cuantizaci√≥n Q4_K_M reduce el tama√±o del modelo manteniendo la calidad
- Este proyecto fue realizado con la asistencia de Github Copilot


