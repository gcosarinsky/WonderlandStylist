# ğŸ© Wonderland Stylist

Un proyecto de IA que transforma texto plano al estilo literario de "Alicia en el PaÃ­s de las Maravillas" de Lewis Carroll, utilizando un modelo LLM optimizado para ejecutarse con recursos de hardware limitados.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un sistema completo de fine-tuning y deployment de un modelo de lenguaje para transformar frases cotidianas en texto con el estilo caracterÃ­stico de Alicia en el PaÃ­s de las Maravillas. El modelo final es ligero y eficiente, diseÃ±ado para funcionar en hardware modesto.

## ğŸ—ï¸ Arquitectura del Proyecto

### 1. GeneraciÃ³n de Datos SintÃ©ticos
- **Modelo**: `llama3.2` mediante Ollama
- **Proceso**: GeneraciÃ³n de pares de texto (normal â†’ estilo Alicia)
- **Salida**: Dataset sintÃ©tico en formato JSONL

### 2. Fine-tuning con LoRA
- **Framework**: Unsloth
- **Modelo base**: Qwen3 0.6B
- **TÃ©cnica**: LoRA (Low-Rank Adaptation)
- **Formato final**: GGUF optimizado (Q4_K_M)

### 3. Inferencia
- **Runtime**: Ollama
- **Interfaz**: Gradio Web UI
- **Modelo**: `Qwen3-0.6B-Base.Q4_K_M_Alicia.gguf`

## ğŸ“ Estructura del Proyecto

```
WonderlandStylist/
â”œâ”€â”€ api/                    # API y aplicaciÃ³n web
â”‚   â””â”€â”€ gradio_app.py      # Interfaz Gradio
â”œâ”€â”€ data/                   # Datos de entrenamiento
â”‚   â”œâ”€â”€ synthetic_alicia.jsonl
â”‚   â”œâ”€â”€ chunks_nltk.txt
â”‚   â”œâ”€â”€ create_chunks.py
â”‚   â””â”€â”€ raw/               # Texto original de Alicia
â”œâ”€â”€ model/                  # Modelos y configuraciÃ³n
â”‚   â”œâ”€â”€ Modelfile.alicia
â”‚   â””â”€â”€ Qwen3-0.6B-Base.Q4_K_M_Alicia.gguf
â”œâ”€â”€ notebooks/              # Notebooks de desarrollo
â”‚   â”œâ”€â”€ notebook_01_gen_synthetic_data.ipynb
â”‚   â””â”€â”€ notebook_02_lora_finetuning.ipynb
â”œâ”€â”€ scripts/                # Scripts de utilidad y testing
â”‚   â”œâ”€â”€ run_gradio_app.py
â”‚   â”œâ”€â”€ test_base_model.py
â”‚   â””â”€â”€ view_dataset.py
â””â”€â”€ fastapi_tests/          # Tests y experimentos con FastAPI
```

### Requisitos
- Python 3.8+
- Ollama instalado
- CUDA (opcional, para aceleraciÃ³n GPU)

## ğŸ’» Uso

### 1. Generar Dataset SintÃ©tico
Ejecutar el script `data/create_chunks.py` para fragmentar el texto original de Alicia usando NLTK. Luego, ejecutar el notebook `notebook_01_gen_synthetic_data.ipynb` para transformar cada fragmento al estilo plano usando llama3.2. Este proceso inverso genera el dataset de entrenamiento en formato instruction-input-output, guardado en `data/synthetic_alicia.jsonl`.

### 2. Fine-tuning del Modelo
Ejecutar el notebook `notebook_02_lora_finetuning.ipynb` para entrenar el modelo Qwen3 con LoRA (se necesita GPU para usar unsloth)

### 3. Cargar el Modelo en Ollama

```bash
cd model
ollama create alicia -f Modelfile.alicia
```

### 4. Ejecutar la Interfaz Gradio

```bash
python scripts/run_gradio_app.py
```

O directamente:

```bash
python api/gradio_app.py
```

## ğŸ§ª Testing

Probar el modelo base:
```bash
python scripts/test_base_model.py
```

Ver el dataset generado:
```bash
python scripts/view_dataset.py
```

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Ollama**: Runtime para modelos LLM
- **Unsloth**: Framework de fine-tuning optimizado
- **Gradio**: Framework para interfaces web interactivas
- **Qwen3 0.6B**: Modelo base ligero
- **LoRA**: TÃ©cnica de fine-tuning eficiente
- **GGUF**: Formato de modelo cuantizado

## ğŸ“Š CaracterÃ­sticas del Modelo

- **TamaÃ±o**: ~400MB (cuantizado Q4_K_M)
- **ParÃ¡metros**: 0.6B
- **Requisitos de RAM**: ~2-4GB

## ğŸ“ Notas

- El modelo estÃ¡ optimizado para espaÃ±ol
- Los datos de entrenamiento provienen del texto original "Alicia en el PaÃ­s de las Maravillas" de Lewis Carroll
- La cuantizaciÃ³n Q4_K_M reduce el tamaÃ±o del modelo manteniendo la calidad

