import gradio as gr
import requests
import json

# Configuraci√≥n de Ollama
OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "alicia:latest"  # Cambia esto por el nombre de tu modelo en Ollama

def generar_respuesta(frase_input, temperatura, max_tokens):
    """
    Genera una respuesta usando el modelo de Ollama con el formato de prompt especial
    """
    # Formato de prompt del entrenamiento (seg√∫n notebook_02_lora_finetuning.ipynb)
    prompt = f"""A continuaci√≥n se muestra una instrucci√≥n que describe una tarea.
Escribe una respuesta que complete adecuadamente la petici√≥n.

### Instrucci√≥n:
Reescribe el siguiente texto con el estilo de Alicia en el Pa√≠s de las Maravillas. 

### Entrada:
{frase_input}

### Respuesta:
"""
    
    # Payload para Ollama
    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": temperatura,
            "num_predict": max_tokens,
            "top_p": 0.9,
            "stop": ["###", "\n\n"]  # Detener en estos tokens
        }
    }
    
    try:
        # Llamada a Ollama
        response = requests.post(OLLAMA_URL, json=payload, timeout=60)
        response.raise_for_status()
        
        result = response.json()
        respuesta = result.get('response', '').strip()
        
        # Limpiar la respuesta si contiene el formato de prompt
        if "### Respuesta:" in respuesta:
            respuesta = respuesta.split("### Respuesta:")[-1].strip()
        
        return respuesta if respuesta else "‚ö†Ô∏è El modelo no gener√≥ ninguna respuesta"
        
    except requests.exceptions.ConnectionError:
        return "‚ùå Error: No se pudo conectar con Ollama. Aseg√∫rate de que est√° corriendo (ollama serve)"
    except requests.exceptions.Timeout:
        return "‚è±Ô∏è Error: La solicitud tard√≥ demasiado tiempo"
    except Exception as e:
        return f"‚ùå Error inesperado: {str(e)}"

def verificar_modelo():
    """
    Verifica que el modelo est√© disponible en Ollama
    """
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        models = response.json().get('models', [])
        model_names = [m['name'] for m in models]
        
        if MODEL_NAME in model_names:
            return f"‚úÖ Modelo '{MODEL_NAME}' disponible"
        else:
            return f"‚ö†Ô∏è Modelo '{MODEL_NAME}' no encontrado. Modelos disponibles: {', '.join(model_names)}"
    except:
        return "‚ùå Ollama no est√° corriendo. Ejecuta: ollama serve"

# Crear la interfaz de Gradio
with gr.Blocks(title="Wonderland Stylist üé©üê∞", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # üé© Wonderland Stylist üê∞
        ### Transforma tu texto al estilo de *Alicia en el Pa√≠s de las Maravillas*
        
        Escribe una frase simple y el modelo la reescribir√° con el estilo fantasioso de Lewis Carroll.
        """
    )
    
    # Estado del modelo
    with gr.Row():
        estado_modelo = gr.Textbox(
            label="Estado del Modelo",
            value=verificar_modelo(),
            interactive=False
        )
    
    with gr.Row():
        with gr.Column(scale=1):
            # Input del usuario
            input_text = gr.Textbox(
                label="‚úèÔ∏è Texto de entrada",
                placeholder="Ejemplo: Era una tarde soleada y los ni√±os jugaban en el parque",
                lines=3
            )
            
            # Controles de configuraci√≥n
            with gr.Accordion("‚öôÔ∏è Configuraci√≥n del Modelo", open=False):
                temperatura = gr.Slider(
                    minimum=0.1,
                    maximum=2.0,
                    value=0.7,
                    step=0.1,
                    label="üå°Ô∏è Temperatura",
                    info="Mayor temperatura = m√°s creatividad (pero menos coherencia)"
                )
                
                max_tokens = gr.Slider(
                    minimum=50,
                    maximum=500,
                    value=150,
                    step=50,
                    label="üìè M√°ximo de tokens",
                    info="Longitud m√°xima de la respuesta"
                )
            
            # Botones
            with gr.Row():
                generar_btn = gr.Button("‚ú® Transformar", variant="primary")
                limpiar_btn = gr.ClearButton([input_text], value="üóëÔ∏è Limpiar")
        
        with gr.Column(scale=1):
            # Output
            output_text = gr.Textbox(
                label="üé® Texto transformado",
                lines=8,
                interactive=False
            )
    
    # Ejemplos
    gr.Examples(
        examples=[
            ["Era una tarde soleada y los ni√±os jugaban en el parque"],
            ["El gato dorm√≠a pl√°cidamente en el sof√°"],
            ["Mar√≠a caminaba por la calle cuando empez√≥ a llover"],
            ["Los estudiantes estudiaban para el examen final"],
            ["El reloj marcaba las cinco de la tarde"]
        ],
        inputs=input_text,
        label="üí° Ejemplos"
    )
    
    # Conectar el bot√≥n con la funci√≥n
    generar_btn.click(
        fn=generar_respuesta,
        inputs=[input_text, temperatura, max_tokens],
        outputs=output_text
    )
    
    gr.Markdown(
        """
        ---
        ### ‚ÑπÔ∏è Informaci√≥n
        - **Modelo:** Fine-tuneado con LoRA sobre Qwen3-0.6B
        - **Dataset:** Fragmentos de "Alicia en el Pa√≠s de las Maravillas"
        - **T√©cnica:** Low-Rank Adaptation (LoRA) con Unsloth
        """
    )

# Lanzar la aplicaci√≥n
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",  # Permite acceso desde la red local
        server_port=7860,
        share=False  # Cambia a True si quieres un link p√∫blico temporal
    )